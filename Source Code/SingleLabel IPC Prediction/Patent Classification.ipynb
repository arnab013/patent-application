{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda8d1e4-f453-43ff-b6eb-25b649bc59a4",
   "metadata": {},
   "source": [
    "# Install Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ecf271-26fd-4af7-8df6-a4640a6331d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to install required packages via pip\n",
    "import os\n",
    "\n",
    "packages = [\n",
    "    'numpy',\n",
    "    'ipywidgets',\n",
    "    'torch',\n",
    "    'matplotlib',\n",
    "    'scikit-learn',\n",
    "    'seaborn',\n",
    "    'transformers',\n",
    "    'datasets',\n",
    "    'evaluate',\n",
    "]\n",
    "\n",
    "# Install each package\n",
    "for package in packages:\n",
    "    os.system(f'pip install {package}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d46abb-9f59-43f2-b9d0-01a561742558",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24fc5751-0432-40f8-963b-b5b4ce59e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import ipywidgets as widgets\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "import ast\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a54e7-3855-4023-88eb-59c1eee8290c",
   "metadata": {},
   "source": [
    "# Check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07faaa86-4c02-49b6-b432-db395d7be6b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470d60a-e796-48ca-8e1c-3649e929be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "epab.fields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae44250-7e33-4deb-bbdf-ae8a1b08283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query for the data that are published in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d64041-b911-4000-b7af-75fe9f475eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q = epab.query_publication(number=\"%\")\n",
    "q = epab.query_publication_language(\"EN\")\n",
    "# getting the results with the application number only\n",
    "q.get_results(\"application.number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341efb78-23cb-4c2a-ad82-dde8493d0836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('q_results_output_2.csv')\n",
    "\n",
    "# Replace '.' with '_' in the column headers\n",
    "df.columns = df.columns.str.replace('.', '_', regex=False)\n",
    "\n",
    "# Display the total number of rows (data entries)\n",
    "total_rows = df.shape[0]\n",
    "print(f\"Total number of data rows: {total_rows}\")\n",
    "\n",
    "# Save the DataFrame with updated column names back to the original file\n",
    "df.to_csv('q_results_output_2.csv', index=False)\n",
    "\n",
    "# Display the updated column names\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d5642c-6f44-455c-b185-6a3569919b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('E:/Code Fest 2024/q_results_output_2.csv')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b0e7d0-33ee-4252-8af3-d2258521f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns you want to keep\n",
    "selected_columns = ['publication_number', 'ipc', 'claims']\n",
    "\n",
    "# Check if the columns exist in the DataFrame\n",
    "existing_columns = [col for col in selected_columns if col in df.columns]\n",
    "\n",
    "# Filter the DataFrame to keep only the selected columns\n",
    "filtered_df = df[existing_columns]\n",
    "\n",
    "# Specify the output file name and the save directory\n",
    "output_file_name = 'selected_columns_output.csv'\n",
    "save_directory = 'E:/Code Fest 2024/'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Set the full path for saving the CSV file\n",
    "full_path = os.path.join(save_directory, output_file_name)\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "filtered_df.to_csv(full_path, index=False)\n",
    "\n",
    "# Print the file path for confirmation\n",
    "print(f\"CSV file '{output_file_name}' generated and saved to: {full_path}\")\n",
    "\n",
    "# Optionally, preview the first few rows of the new CSV file\n",
    "print(\"\\nPreview of the generated CSV file:\")\n",
    "print(filtered_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a136e-fecd-492e-b59f-4bd698644e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('E:/Code Fest 2024/selected_columns_output.csv')\n",
    "\n",
    "# Function to filter claims for English language\n",
    "def filter_english_claims(claims):\n",
    "    claims_list = ast.literal_eval(claims)  # Convert string representation to list of dictionaries\n",
    "    # Filter the claims for language 'EN'\n",
    "    english_claims = [claim for claim in claims_list if claim['language'] == 'EN']\n",
    "    return english_claims\n",
    "\n",
    "# Apply the filter function to the 'claims' column\n",
    "df['english_claims'] = df['claims'].apply(filter_english_claims)\n",
    "\n",
    "# Replace the 'claims' column with 'english_claims' data\n",
    "df['claims'] = df['english_claims']\n",
    "\n",
    "# Optionally drop the 'english_claims' column if no longer needed\n",
    "df = df.drop(columns=['english_claims'])\n",
    "\n",
    "# Save the modified DataFrame to 'filtered_english_claims.csv'\n",
    "df.to_csv('E:/Code Fest 2024/filtered_english_claims.csv', index=False)\n",
    "\n",
    "# Display the modified DataFrame (optional)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d75f9ab",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c4219f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "file_path = 'train_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "model = SentenceTransformer('AI-Growth-Lab/PatentSBERTa')\n",
    "\n",
    "# Remove rows where the 'claims' column is empty (NaN or empty string)\n",
    "df = df[df['claims'].notna() & (df['claims'].str.strip() != '')]\n",
    "\n",
    "# Extract the first letter of the IPC code for section prediction\n",
    "df['ipc_section'] = df['ipc'].apply(lambda x: x[0] if pd.notna(x) and len(x) > 0 else '')\n",
    "\n",
    "# Encode labels for IPC section\n",
    "label_encoder_section = LabelEncoder()\n",
    "df['encoded_section'] = label_encoder_section.fit_transform(df['ipc_section'])\n",
    "\n",
    "# Train/test split (for section prediction)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['claims'], df['encoded_section'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode the claims into embeddings using PatentSBERTa\n",
    "X_train_embeddings = model.encode(X_train.tolist())\n",
    "X_test_embeddings = model.encode(X_test.tolist())\n",
    "\n",
    "# Train a simple classifier (e.g., Logistic Regression) on the embeddings\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Cross-validation to simulate training in folds (like epochs)\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "# Train and evaluate the model for each fold with a progress bar\n",
    "for fold, (train_index, test_index) in enumerate(tqdm(skf.split(X_train_embeddings, y_train), total=skf.get_n_splits(), desc=\"Cross-validation Folds\")):\n",
    "    X_train_fold, X_test_fold = X_train_embeddings[train_index], X_train_embeddings[test_index]\n",
    "    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]  # Use .iloc[] for positional indexing\n",
    "\n",
    "    classifier.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_fold = classifier.predict(X_test_fold)\n",
    "    \n",
    "    # Calculate metrics for the current fold\n",
    "    accuracy = accuracy_score(y_test_fold, y_pred_fold)\n",
    "    precision = precision_score(y_test_fold, y_pred_fold, average='weighted')\n",
    "    recall = recall_score(y_test_fold, y_pred_fold, average='weighted')\n",
    "    f1 = f1_score(y_test_fold, y_pred_fold, average='weighted')\n",
    "\n",
    "    # Append metrics to lists\n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "\n",
    "    # Display progress for each fold\n",
    "    print(f\"Fold {fold+1}/{skf.get_n_splits()} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "# Save the final classifier and label encoder after cross-validation training is done\n",
    "classifier_file_path = 'Models/ipc_section_classifier.pkl'\n",
    "joblib.dump(classifier, classifier_file_path)\n",
    "print(f\"Classifier model saved to {classifier_file_path}\")\n",
    "\n",
    "label_encoder_file_path = 'Models/ipc_section_label_encoder.pkl'\n",
    "joblib.dump(label_encoder_section, label_encoder_file_path)\n",
    "print(f\"Label encoder saved to {label_encoder_file_path}\")\n",
    "\n",
    "# Visualizing Metrics\n",
    "epochs = range(1, len(accuracy_list) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(epochs, accuracy_list, label='Accuracy', marker='o')\n",
    "plt.plot(epochs, precision_list, label='Precision', marker='o')\n",
    "plt.plot(epochs, recall_list, label='Recall', marker='o')\n",
    "plt.plot(epochs, f1_list, label='F1-score', marker='o')\n",
    "plt.xlabel('Fold (Simulating Epochs)')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Training Metrics Over Cross-Validation Folds')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Final evaluation on the test set with progress bar\n",
    "print(\"\\nTraining final classifier on full training set...\")\n",
    "with tqdm(total=len(X_train_embeddings), desc=\"Final Training Progress\") as pbar:\n",
    "    classifier.fit(X_train_embeddings, y_train)\n",
    "    pbar.update(len(X_train_embeddings))\n",
    "\n",
    "y_pred = classifier.predict(X_test_embeddings)\n",
    "\n",
    "# Compute final metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Test Precision: {precision}\")\n",
    "print(f\"Test Recall: {recall}\")\n",
    "print(f\"Test F1-score: {f1}\")\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder_section.classes_))\n",
    "\n",
    "# Confusion matrix for final evaluation\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder_section.classes_, yticklabels=label_encoder_section.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for IPC Section Prediction')\n",
    "plt.show()\n",
    "\n",
    "# Save the final classifier after the test evaluation\n",
    "final_classifier_file_path = 'Models/final_ipc_section_classifier.pkl'\n",
    "joblib.dump(classifier, final_classifier_file_path)\n",
    "print(f\"Final classifier model saved to {final_classifier_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6112da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained classifier\n",
    "classifier_file_path = 'Models/ipc_section_classifier.pkl'\n",
    "joblib.dump(classifier, classifier_file_path)\n",
    "\n",
    "# Save the label encoder for the IPC sections\n",
    "label_encoder_file_path = 'Models/ipc_section_label_encoder.pkl'\n",
    "joblib.dump(label_encoder_section, label_encoder_file_path)\n",
    "\n",
    "print(f\"Model saved to {classifier_file_path}\")\n",
    "print(f\"Label encoder saved to {label_encoder_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e7dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the evaluation dataset\n",
    "eval_file_path = 'eval_dataset.csv'\n",
    "eval_df = pd.read_csv(eval_file_path)\n",
    "\n",
    "# Check if the 'ipc' column exists\n",
    "if 'ipc' in eval_df.columns:\n",
    "    # Ensure the 'ipc' column is treated as a string, even if it's not\n",
    "    eval_df['ipc'] = eval_df['ipc'].astype(str)\n",
    "\n",
    "    # Extract the IPC section from the 'ipc' column (first character)\n",
    "    eval_df['ipc_section'] = eval_df['ipc'].apply(lambda x: x[0] if pd.notna(x) and len(x) > 0 else '')\n",
    "    \n",
    "    # Verify the 'ipc_section' column was created correctly\n",
    "    print(\"First few rows of 'ipc_section' column:\")\n",
    "    print(eval_df[['ipc', 'ipc_section']].head())\n",
    "\n",
    "    # Save the updated dataset back to the original file (overwriting it)\n",
    "    eval_df.to_csv(eval_file_path, index=False)\n",
    "    \n",
    "    print(f\"IPC sections added successfully and saved back to {eval_file_path}\")\n",
    "else:\n",
    "    print(\"Error: The 'ipc' column is missing from the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file_path = 'eval_dataset.csv'\n",
    "eval_df = pd.read_csv(eval_file_path)\n",
    "print(eval_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268cfe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and encoders\n",
    "classifier = joblib.load('Models/ipc_section_classifier.pkl')\n",
    "label_encoder_section = joblib.load('Models/ipc_section_label_encoder.pkl')\n",
    "\n",
    "# Load the evaluation dataset\n",
    "eval_file_path = 'eval_dataset.csv'\n",
    "eval_df = pd.read_csv(eval_file_path)\n",
    "\n",
    "# Initialize the SentenceTransformer model (PatentSBERTa)\n",
    "model = SentenceTransformer('AI-Growth-Lab/PatentSBERTa')\n",
    "\n",
    "# Remove rows where the 'claims' column is empty (NaN or empty string)\n",
    "eval_df = eval_df[eval_df['claims'].notna() & (eval_df['claims'].str.strip() != '')]\n",
    "\n",
    "# Encode the 'claims' from eval_dataset.csv into embeddings\n",
    "eval_claims_embeddings = model.encode(eval_df['claims'].tolist())\n",
    "\n",
    "# Use the trained classifier to predict the IPC section\n",
    "predicted_encoded_sections = classifier.predict(eval_claims_embeddings)\n",
    "\n",
    "# Decode the predicted sections back to their original IPC section format\n",
    "predicted_sections = label_encoder_section.inverse_transform(predicted_encoded_sections)\n",
    "\n",
    "# Add the predicted sections and check if predictions are correct\n",
    "eval_df['predicted_section'] = predicted_sections\n",
    "eval_df['correct_prediction'] = eval_df['predicted_section'] == eval_df['ipc_section']\n",
    "\n",
    "# Display the DataFrame showing publication_number, ipc, claims, actual ipc_section, and predicted_section\n",
    "result_df = eval_df[['publication_number', 'ipc', 'claims', 'ipc_section', 'predicted_section', 'correct_prediction']]\n",
    "\n",
    "# Display the DataFrame with the results\n",
    "print(result_df.head(10))  # Show first 10 rows\n",
    "\n",
    "# Calculate and display the accuracy\n",
    "correct_predictions = result_df['correct_prediction'].sum()\n",
    "total_predictions = len(result_df)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(f\"\\nTotal predictions: {total_predictions}\")\n",
    "print(f\"Correct predictions: {correct_predictions}\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "result_file_path = 'predicted_eval_dataset.csv'\n",
    "result_df.to_csv(result_file_path, index=False)\n",
    "\n",
    "print(f\"\\nResults saved to {result_file_path}\")\n",
    "\n",
    "# Optional: Show the first 10 rows of the saved file\n",
    "result_df.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a77aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved classifier and label encoder\n",
    "classifier_file_path = 'Models/ipc_section_classifier.pkl'\n",
    "label_encoder_file_path = 'Models/ipc_section_label_encoder.pkl'\n",
    "\n",
    "loaded_classifier = joblib.load(classifier_file_path)\n",
    "loaded_label_encoder = joblib.load(label_encoder_file_path)\n",
    "\n",
    "# Load the SentenceTransformer model for embeddings\n",
    "model = SentenceTransformer('AI-Growth-Lab/PatentSBERTa')\n",
    "\n",
    "# Function to take user input and predict the IPC section\n",
    "def predict_ipc_section():\n",
    "    while True:\n",
    "        # Take user input (claims)\n",
    "        user_input = input(\"Enter the claims (or type 'exit' to quit): \")\n",
    "        \n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Exiting the prediction prompt.\")\n",
    "            break\n",
    "        \n",
    "        # Encode the input claims into embeddings\n",
    "        claims_embedding = model.encode([user_input])\n",
    "\n",
    "        # Predict using the loaded classifier\n",
    "        predicted_encoded_section = loaded_classifier.predict(claims_embedding)\n",
    "\n",
    "        # Decode the predicted section back to the IPC format\n",
    "        predicted_section = loaded_label_encoder.inverse_transform(predicted_encoded_section)\n",
    "\n",
    "        # Show the prediction\n",
    "        print(f\"Predicted IPC Section: {predicted_section[0]}\")\n",
    "\n",
    "# Start the interactive prompt\n",
    "predict_ipc_section()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
