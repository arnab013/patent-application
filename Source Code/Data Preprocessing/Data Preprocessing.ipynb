{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda8d1e4-f453-43ff-b6eb-25b649bc59a4",
   "metadata": {},
   "source": [
    "# Install Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ecf271-26fd-4af7-8df6-a4640a6331d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to install required packages via pip\n",
    "import os\n",
    "\n",
    "packages = [\n",
    "    'numpy',\n",
    "    'ipywidgets',\n",
    "    'torch',\n",
    "    'matplotlib',\n",
    "    'scikit-learn',\n",
    "    'seaborn',\n",
    "    'transformers',\n",
    "    'datasets',\n",
    "    'evaluate',\n",
    "]\n",
    "\n",
    "# Install each package\n",
    "for package in packages:\n",
    "    os.system(f'pip install {package}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d46abb-9f59-43f2-b9d0-01a561742558",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "24fc5751-0432-40f8-963b-b5b4ce59e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "import ast\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8824098f-683e-4560-bd95-b8ffc45b0d7f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992b04c6-f4a3-4013-9979-03c1d521ef08",
   "metadata": {},
   "source": [
    "## Import Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e8848eca-6387-4796-85bf-53fc783b773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from epo.tipdata.epab import EPABClient\n",
    "#For test database (10K)\n",
    "#epab = EPABClient(env='TEST')\n",
    "\n",
    "#For the complete database\n",
    "epab = EPABClient(env='PROD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60472b2e-8f2a-453b-818b-27fa0fc30e58",
   "metadata": {},
   "source": [
    "## Check EPAB fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470d60a-e796-48ca-8e1c-3649e929be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "epab.fields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae44250-7e33-4deb-bbdf-ae8a1b08283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = epab.query_publication(number=\"%\", kind_code=\"\", date=\"20240101-20240131\", language='EN')\n",
    "print ('Total query:', q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd358f7a-1c4c-48e1-97d6-eab28cd41bfe",
   "metadata": {},
   "source": [
    "## Create a CSV file to store the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a4593-7ef3-4711-b418-774d5c097b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = q.get_results(\"publication.number, ipc, claims\",limit = len(q))\n",
    "df = pd.DataFrame(result)\n",
    "csv_file = \"q_results_output.csv\"\n",
    "df.to_csv(csv_file)\n",
    "print(f\"All data fetched and saved to {csv_file} successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ed86a9-18a1-4a0c-b2bf-dcf13734700e",
   "metadata": {},
   "source": [
    "## Modify the column header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341efb78-23cb-4c2a-ad82-dde8493d0836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('q_results_output.csv')\n",
    "\n",
    "# Replace '.' with '_' in the column headers\n",
    "df.columns = df.columns.str.replace('.', '_', regex=False)\n",
    "\n",
    "# Save the DataFrame with updated column names back to the original file\n",
    "df.to_csv('q_results_output.csv', index=False)\n",
    "\n",
    "# Display the updated column names\n",
    "print(df.columns)\n",
    "\n",
    "# Display the total number of rows (data entries)\n",
    "total_rows = df.shape[0]\n",
    "print(f\"Total number of data rows: {total_rows}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a302d101-923d-4ffe-bac1-56cca8463fad",
   "metadata": {},
   "source": [
    "## Generate Data for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d5642c-6f44-455c-b185-6a3569919b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('q_results_output.csv')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b0e7d0-33ee-4252-8af3-d2258521f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns you want to keep\n",
    "selected_columns = ['publication_number', 'ipc', 'claims']\n",
    "\n",
    "# Check if the columns exist in the DataFrame\n",
    "existing_columns = [col for col in selected_columns if col in df.columns]\n",
    "\n",
    "# Filter the DataFrame to keep only the selected columns\n",
    "filtered_df = df[existing_columns]\n",
    "\n",
    "# Specify the output file name and the save directory\n",
    "output_file_name = 'selected_columns_output.csv'\n",
    "save_directory = './'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Set the full path for saving the CSV file\n",
    "full_path = os.path.join(save_directory, output_file_name)\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "filtered_df.to_csv(full_path, index=False)\n",
    "\n",
    "# Print the file path for confirmation\n",
    "print(f\"CSV file '{output_file_name}' generated and saved to: {full_path}\")\n",
    "\n",
    "# Optionally, preview the first few rows of the new CSV file\n",
    "print(\"\\nPreview of the generated CSV file:\")\n",
    "print(filtered_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15265ff9-cca3-4ab4-97aa-a4b39d2e1c6b",
   "metadata": {},
   "source": [
    "## Separate claim data in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a136e-fecd-492e-b59f-4bd698644e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('selected_columns_output.csv')\n",
    "\n",
    "# Function to filter claims for English language\n",
    "def filter_english_claims(claims):\n",
    "    claims_list = ast.literal_eval(claims)  # Convert string representation to list of dictionaries\n",
    "    # Filter the claims for language 'EN'\n",
    "    english_claims = [claim for claim in claims_list if claim['language'] == 'EN']\n",
    "    return english_claims\n",
    "\n",
    "# Apply the filter function to the 'claims' column\n",
    "df['english_claims'] = df['claims'].apply(filter_english_claims)\n",
    "\n",
    "# Replace the 'claims' column with 'english_claims' data\n",
    "df['claims'] = df['english_claims']\n",
    "\n",
    "# Optionally drop the 'english_claims' column if no longer needed\n",
    "df = df.drop(columns=['english_claims'])\n",
    "\n",
    "# Save the modified DataFrame to 'filtered_english_claims.csv'\n",
    "df.to_csv('filtered_english_claims.csv', index=False)\n",
    "\n",
    "# Display the modified DataFrame (optional)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ae688-9709-407e-9ff2-86a5b12b4da2",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36188d21-8109-46ab-993d-256977aef3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = 'filtered_english_claims.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Display the first three rows in an interactive DataTable\n",
    "    table_widget = widgets.Output()\n",
    "    with table_widget:\n",
    "        display(df.head(3))\n",
    "\n",
    "    display(table_widget)\n",
    "else:\n",
    "    print(f\"The file {file_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3e6f7f06-4a55-46db-ac2a-310ba8a76e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the 'IPC' column\n",
    "def clean_ipc_column(ipc_data):\n",
    "    try:\n",
    "        # Convert string representation of list to actual list of dictionaries\n",
    "        ipc_list = ast.literal_eval(ipc_data)\n",
    "        \n",
    "        # Extract and clean the 'symbol', removing unwanted characters except alphanumeric and '/'\n",
    "        cleaned_ipc = [re.sub(r'[^A-Za-z0-9/]', '', entry['symbol']) for entry in ipc_list]\n",
    "        \n",
    "        # Join cleaned symbols into a single string, separated by commas\n",
    "        return ', '.join(cleaned_ipc)\n",
    "    except (ValueError, SyntaxError, KeyError):\n",
    "        # If any error occurs during the processing, return the original data\n",
    "        return ipc_data\n",
    "\n",
    "# Example usage:\n",
    "df['cleaned_ipc'] = df['ipc'].apply(clean_ipc_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "552acbcd-9631-4dde-8b1d-33199391e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the 'Claims' column and convert to lowercase\n",
    "def clean_claims_column(claims_data):\n",
    "    try:\n",
    "        # Convert string representation to a list of dictionaries\n",
    "        claims_list = ast.literal_eval(claims_data)\n",
    "        \n",
    "        # Extract and clean the 'text' field where the language is 'EN'\n",
    "        for claim in claims_list:\n",
    "            if claim.get('language') == 'EN':\n",
    "                # Remove HTML tags and claim numbers\n",
    "                cleaned_claims = re.sub(r'<.*?>', '', claim['text'])  # Remove HTML tags\n",
    "                cleaned_claims = re.sub(r'\\b\\d+\\.\\s', '', cleaned_claims)  # Remove claim numbers like 1., 2., etc.\n",
    "                \n",
    "                # Remove unwanted special characters but keep commas, periods, and question marks\n",
    "                cleaned_claims = re.sub(r'[^a-zA-Z0-9,.?\\s]', '', cleaned_claims)\n",
    "                \n",
    "                # Remove newline characters\n",
    "                cleaned_claims = cleaned_claims.replace('\\n', '')\n",
    "\n",
    "                # Convert the cleaned claims to lowercase\n",
    "                cleaned_claims = cleaned_claims.lower()\n",
    "\n",
    "                return cleaned_claims.strip()  # Return cleaned and lowercased text with extra spaces removed\n",
    "                \n",
    "        # Return original data if no 'EN' claims found\n",
    "        return claims_data\n",
    "    \n",
    "    except (ValueError, SyntaxError, KeyError):\n",
    "        # Return original data if any error occurs\n",
    "        return claims_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7002298b-90ad-4137-8a8e-d570458910d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the 'description_text' column\n",
    "def clean_description_column(description_text):\n",
    "    if not isinstance(description_text, str):\n",
    "        description_text = ''  # Convert non-string or NaN to an empty string\n",
    "    cleaned_description = re.sub(r'<.*?>', '', description_text)  # Remove all HTML-like tags\n",
    "    cleaned_description = re.sub(r'\\s+', ' ', cleaned_description).strip()  # Clean up extra spaces and new lines\n",
    "    return cleaned_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "86cef80c-e711-49c7-a2e2-6ef63cae6c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the abstract column \n",
    "def clean_abstract_column(abstract_text):\n",
    "    if not isinstance(abstract_text, str):\n",
    "        return ''  # Return empty string if the input is not a valid string\n",
    "    \n",
    "    # Step 1: Remove image tags and their associated metadata like <img ... />\n",
    "    cleaned_abstract = re.sub(r'<img.*?>', '', abstract_text)\n",
    "\n",
    "    # Step 2: Remove all other HTML tags like <p>, <ul>, <li>, <br>, etc.\n",
    "    cleaned_abstract = re.sub(r'<.*?>', ' ', cleaned_abstract)\n",
    "\n",
    "    # Step 3: Remove list-related numbering like (1), (2), (3), etc. and also numbers with dots 1., 2., 3., etc.\n",
    "    cleaned_abstract = re.sub(r'\\(\\d+\\)', '', cleaned_abstract)  # Remove list numbering like (1), (2), etc.\n",
    "    cleaned_abstract = re.sub(r'\\b\\d+\\.\\s', '', cleaned_abstract)  # Remove numbering like 1. 2. etc.\n",
    "    \n",
    "    # Step 4: Remove extra whitespace and newlines\n",
    "    cleaned_abstract = re.sub(r'\\s+', ' ', cleaned_abstract).strip()\n",
    "\n",
    "    return cleaned_abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be7fb57-631b-470a-bd63-bec24feb0c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths\n",
    "input_file_path = 'filtered_english_claims.csv'\n",
    "output_file_path = 'cleaned_columns_output.csv'\n",
    "\n",
    "# Set the chunk size for reading and processing the CSV file\n",
    "chunk_size = 5000\n",
    "\n",
    "# Process the CSV file in chunks\n",
    "chunks = pd.read_csv(input_file_path, chunksize=chunk_size)\n",
    "\n",
    "# Initialize the output CSV file with headers from the first chunk\n",
    "header_written = False\n",
    "\n",
    "for chunk in chunks:\n",
    "    # Apply the cleaning functions to the relevant columns\n",
    "    chunk['ipc'] = chunk['ipc'].apply(clean_ipc_column)\n",
    "    chunk['claims'] = chunk['claims'].apply(clean_claims_column)\n",
    "\n",
    "    # Optionally add additional cleaning for 'description_text' and 'abstract_text'\n",
    "    # chunk['description_text'] = chunk['description_text'].apply(clean_description_column)\n",
    "    # chunk['abstract_text'] = chunk['abstract_text'].apply(clean_abstract_column)\n",
    "\n",
    "    # Keep only the 'ipc' and 'claims' columns\n",
    "    chunk = chunk[['publication_number', 'ipc', 'claims']]\n",
    "\n",
    "    # Write the cleaned chunk to the output CSV file\n",
    "    if not header_written:\n",
    "        chunk.to_csv(output_file_path, index=False, mode='w')\n",
    "        header_written = True\n",
    "    else:\n",
    "        chunk.to_csv(output_file_path, index=False, mode='a', header=False)\n",
    "\n",
    "# Print a success message\n",
    "print(f\"Cleaned data saved successfully to {output_file_path}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f671870-0cfc-4d80-ba00-47e13992c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = 'cleaned_columns_output.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Display the first three rows in an interactive DataTable\n",
    "    table_widget = widgets.Output()\n",
    "    with table_widget:\n",
    "        display(df.head(20))\n",
    "\n",
    "    display(table_widget)\n",
    "else:\n",
    "    print(f\"The file {file_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13add8b5-bc52-449d-8fee-1aa310802ff2",
   "metadata": {},
   "source": [
    "## Load Data Randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94868249-22e8-4c17-a965-79dfb73a3c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "file_path = 'cleaned_columns_output.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the shape of the loaded dataset to verify\n",
    "print(f\"Loaded {df.shape[0]} rows from the dataset.\")\n",
    "\n",
    "# Save the accumulated data to 'even_data_distribution.csv'\n",
    "output_file_path = 'random_data_distribution.csv'\n",
    "df.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88043cb5-d08a-4a91-925c-bd366d342b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the IPC section (first letter of the IPC code)\n",
    "df['ipc_section'] = df['ipc'].apply(lambda x: x[0] if pd.notna(x) and len(x) > 0 else '')\n",
    "\n",
    "# Count the number of rows for each IPC section\n",
    "section_counts = df['ipc_section'].value_counts()\n",
    "\n",
    "# Print the counts for each section\n",
    "print(\"Data count per section:\")\n",
    "print(section_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b3ab9-2015-4247-a484-30cc48656fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of rows per section\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=section_counts.index, y=section_counts.values, hue=section_counts.index, palette='viridis', dodge=False, legend=False)\n",
    "plt.title('Distribution of Rows by IPC Section (First 20,000 Rows)')\n",
    "plt.xlabel('IPC Section')\n",
    "plt.ylabel('Number of Rows')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4faacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'random_data_distribution.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Display few rows in an interactive DataTable\n",
    "    table_widget = widgets.Output()\n",
    "    with table_widget:\n",
    "        display(df.head(20))\n",
    "\n",
    "    display(table_widget)\n",
    "else:\n",
    "    print(f\"The file {file_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0054c186-bd68-4307-92fe-eced85acaabb",
   "metadata": {},
   "source": [
    "## Load Data with even distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff40cc2-4251-4130-8c36-7dbdca836dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to the cleaned dataset\n",
    "file_path = 'cleaned_columns_output.csv'\n",
    "\n",
    "# Initialize an empty DataFrame to store the accumulated data\n",
    "accumulated_data = pd.DataFrame()\n",
    "\n",
    "# Define the IPC sections (A, B, C, D, E, F, G, H)\n",
    "sections = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
    "\n",
    "# Initialize a dictionary to track the number of rows per section\n",
    "section_counts = {section: 0 for section in sections}\n",
    "\n",
    "# Desired number of rows per section\n",
    "max_rows_per_section = 5000\n",
    "\n",
    "# Function to check if 'claims' is empty ([], NaN, or empty string)\n",
    "def is_valid_claim(claims):\n",
    "    try:\n",
    "        # Ensure proper type and check for empty lists\n",
    "        claims_list = ast.literal_eval(claims) if isinstance(claims, str) else claims\n",
    "        if isinstance(claims_list, list) and len(claims_list) == 0:\n",
    "            return False\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "    return pd.notna(claims) and claims.strip() != ''  # Ensure no empty strings or NaN\n",
    "\n",
    "# Load data in chunks and accumulate data until we have enough rows for each section\n",
    "chunksize = 500  # Adjust the chunk size if necessary\n",
    "\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "    # Remove duplicates based on the 'publication_number' column (assuming this column exists)\n",
    "    chunk = chunk.drop_duplicates(subset=['publication_number']).copy()\n",
    "    \n",
    "    # Remove rows where the 'claims' column is empty (i.e., NaN, empty string, or [])\n",
    "    chunk = chunk[chunk['claims'].apply(is_valid_claim)].copy()\n",
    "\n",
    "    # Extract the first letter of the IPC code to create the section\n",
    "    chunk['ipc_section'] = chunk['ipc'].apply(lambda x: x[0] if pd.notna(x) and len(x) > 0 else '')\n",
    "\n",
    "    # Iterate over each section and accumulate rows\n",
    "    for section in sections:\n",
    "        if section_counts[section] < max_rows_per_section:\n",
    "            section_data = chunk[chunk['ipc_section'] == section]\n",
    "            required_rows = max_rows_per_section - section_counts[section]\n",
    "\n",
    "            # Sample up to the required number of rows from the current chunk\n",
    "            sampled_section_data = section_data.head(required_rows)\n",
    "\n",
    "            # Concatenate the sampled data to the accumulated data\n",
    "            accumulated_data = pd.concat([accumulated_data, sampled_section_data])\n",
    "\n",
    "            # Update the section count\n",
    "            section_counts[section] += len(sampled_section_data)\n",
    "\n",
    "        # If we have reached the required rows for all sections, stop loading more data\n",
    "        if all(count >= max_rows_per_section for count in section_counts.values()):\n",
    "            break\n",
    "\n",
    "    # Check again if we have enough rows for all sections, stop if true\n",
    "    if all(count >= max_rows_per_section for count in section_counts.values()):\n",
    "        break\n",
    "\n",
    "# Print the counts for each section\n",
    "print(\"Data count per section:\")\n",
    "print(section_counts)\n",
    "\n",
    "# Display the shape of the accumulated dataset\n",
    "print(f\"Accumulated dataset contains {accumulated_data.shape[0]} rows.\")\n",
    "\n",
    "# Save the accumulated data to 'even_data_distribution.csv'\n",
    "output_file_path = 'even_data_distribution.csv'\n",
    "accumulated_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Data has been saved to {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3daa3e8-e933-457e-84e0-9db1b18780b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of rows per section\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(section_counts.keys()), y=list(section_counts.values()), hue=list(section_counts.keys()), palette='viridis', dodge=False, legend=False)\n",
    "plt.title('Distribution of Rows by IPC Section')\n",
    "plt.xlabel('IPC Section')\n",
    "plt.ylabel('Number of Rows')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c1aafd-4eb9-4d5e-88ce-c63ce528a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "file_path = 'even_data_distribution.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the shape of the loaded dataset to verify\n",
    "print(f\"Loaded {df.shape[0]} rows from the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69c9b7-6726-4820-b89f-b6aea5862196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Display few rows in an interactive DataTable\n",
    "    table_widget = widgets.Output()\n",
    "    with table_widget:\n",
    "        display(df.head(20))\n",
    "\n",
    "    display(table_widget)\n",
    "else:\n",
    "    print(f\"The file {file_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed50564c-36d7-459c-a6ec-4bdb9922301f",
   "metadata": {},
   "source": [
    "## Generate Data for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ee7778-4ee9-48de-b43b-c4f19de2faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "#file_path = 'even_data_distribution.csv' # For even dataset\n",
    "file_path = 'random_data_distribution.csv' # For random dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'ipc' column to string type to avoid TypeError\n",
    "df['ipc'] = df['ipc'].astype(str)\n",
    "\n",
    "# Split the dataset into training and evaluation sets (e.g., 80% for training and 20% for evaluation)\n",
    "train_df, eval_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Save the training and evaluation datasets to new CSV files\n",
    "train_file_path = 'train_dataset.csv'\n",
    "eval_file_path = 'eval_dataset.csv'\n",
    "\n",
    "train_df.to_csv(train_file_path, index=False)\n",
    "eval_df.to_csv(eval_file_path, index=False)\n",
    "\n",
    "# Print success message\n",
    "print(f\"Training dataset saved to {train_file_path}\")\n",
    "print(f\"Evaluation dataset saved to {eval_file_path}\")\n",
    "\n",
    "# Optionally, display the sizes of the datasets to verify the split\n",
    "print(f\"Training dataset size: {train_df.shape[0]} rows\")\n",
    "print(f\"Evaluation dataset size: {eval_df.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb0358e-69da-4685-a3f3-dc5145dde7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new dataset and display it\n",
    "new_file_path = 'train_dataset.csv'\n",
    "df = pd.read_csv(new_file_path)\n",
    "print(df.head())\n",
    "\n",
    "# Analyze the data\n",
    "print(\"\\nData Information:\\n\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e243a34-b8a6-4e75-815a-d2f85cf7f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing Values in Each Column:\\n\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e7dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the evaluation dataset\n",
    "eval_file_path = 'eval_dataset.csv'\n",
    "eval_df = pd.read_csv(eval_file_path)\n",
    "\n",
    "# Check if the 'ipc' column exists\n",
    "if 'ipc' in eval_df.columns:\n",
    "    # Ensure the 'ipc' column is treated as a string, even if it's not\n",
    "    eval_df['ipc'] = eval_df['ipc'].astype(str)\n",
    "\n",
    "    # Extract the IPC section from the 'ipc' column (first character)\n",
    "    eval_df['ipc_section'] = eval_df['ipc'].apply(lambda x: x[0] if pd.notna(x) and len(x) > 0 else '')\n",
    "    \n",
    "    # Verify the 'ipc_section' column was created correctly\n",
    "    print(\"First few rows of 'ipc_section' column:\")\n",
    "    print(eval_df[['ipc', 'ipc_section']].head())\n",
    "\n",
    "    # Save the updated dataset back to the original file (overwriting it)\n",
    "    eval_df.to_csv(eval_file_path, index=False)\n",
    "    \n",
    "    print(f\"IPC sections added successfully and saved back to {eval_file_path}\")\n",
    "else:\n",
    "    print(\"Error: The 'ipc' column is missing from the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file_path = 'eval_dataset.csv'\n",
    "eval_df = pd.read_csv(eval_file_path)\n",
    "print(eval_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bdb3f2-4e43-43b6-8262-be76bd20a2d0",
   "metadata": {},
   "source": [
    "# Create dataset for prior art search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3670decc-3061-425f-8728-0af5f0d00eb9",
   "metadata": {},
   "source": [
    "## Query Data and save in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "540f5142-e5a2-48c4-8540-351c0e37a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from epo.tipdata.epab import EPABClient\n",
    "#For test database (10K)\n",
    "#epab = EPABClient(env='TEST')\n",
    "\n",
    "#For the complete database\n",
    "epab = EPABClient(env='PROD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30709d28-f9cf-4527-831d-296fd379df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query for the data that are published in English since Jan 2021 to Jan 2024 \n",
    "c = epab.query_publication(number=\"%\", kind_code=\"A1\", date=\"20240101-20240131\", language='EN')\n",
    "print ('Total number of query:', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d153bada-1d5c-4e64-8dbb-840f6668a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "search_info = c.get_results(\n",
    "    \"publication.number, application.filing_date, ipc, search_report.date, search_report.ipc_field, search_report.is_no_unity, search_report.is_incomplete_search, search_report.is_no_search, srep_citation.is_patent, srep_citation.document, srep_citation.document_xml, srep_citation.category, srep_citation.relevant_claims, srep_citation.relevant_passage, srep_citation.corresponding_docs\",\n",
    "limit = 50000)\n",
    "df_search = pd.DataFrame(search_info)\n",
    "df_search.to_csv('search_report_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a54887-c4f3-4399-90d9-435d99a7aa02",
   "metadata": {},
   "source": [
    "## Modify Column Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08598069-efc9-4a4c-9fea-759c1b7570c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('search_report_info.csv')\n",
    "\n",
    "# Replace '.' with '_' in the column headers\n",
    "df.columns = df.columns.str.replace('.', '_', regex=False)\n",
    "\n",
    "# Display the total number of rows (data entries)\n",
    "total_rows = df.shape[0]\n",
    "print(f\"Total number of data rows: {total_rows}\")\n",
    "\n",
    "# Save the DataFrame with updated column names back to the original file\n",
    "df.to_csv('search_report_info.csv', index=False)\n",
    "\n",
    "# Display the updated column names\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121c9fee-b9c3-4535-a6fe-96d292ef9005",
   "metadata": {},
   "source": [
    "## Select Data Fields to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6f1d7-3c53-449c-9ef7-901de2fb390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns you want to keep\n",
    "selected_columns = ['publication_number', 'ipc', 'search_report_ipc_field', 'search_report_is_no_unity', 'search_report_is_incomplete_search',\n",
    "       'search_report_is_no_search', 'srep_citation']\n",
    "\n",
    "# Check if the columns exist in the DataFrame\n",
    "existing_columns = [col for col in selected_columns if col in df.columns]\n",
    "\n",
    "# Filter the DataFrame to keep only the selected columns\n",
    "filtered_df = df[existing_columns]\n",
    "\n",
    "# Specify the output file name and the save directory\n",
    "output_file_name = 'selected_columns_output.csv'\n",
    "save_directory = './'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Set the full path for saving the CSV file\n",
    "full_path = os.path.join(save_directory, output_file_name)\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "filtered_df.to_csv(full_path, index=False)\n",
    "\n",
    "# Print the file path for confirmation\n",
    "print(f\"CSV file '{output_file_name}' generated and saved to: {full_path}\")\n",
    "\n",
    "# Optionally, preview the first few rows of the new CSV file\n",
    "print(\"\\nPreview of the generated CSV file:\")\n",
    "print(filtered_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636c5910-f979-4190-afe0-acaac124f2ad",
   "metadata": {},
   "source": [
    "## Remove Data with Empty fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d700f66-7da4-48cd-a152-221b600f9f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path (replace with your actual file path)\n",
    "file_path = 'selected_columns_output.csv'\n",
    "output_file_path = 'filtered_search_report_info.csv'  # File path for the filtered CSV\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Filter out rows where 'srep_citation' is empty (i.e., equals '[]')\n",
    "srep_citation_filtered = data[data['srep_citation'] != '[]']\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "srep_citation_filtered.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Print confirmation and the header\n",
    "print(f\"Filtered data saved to {output_file_path}\")\n",
    "print(\"Filtered DataFrame Header:\", srep_citation_filtered.columns)\n",
    "\n",
    "# Display the total number of rows (data entries)\n",
    "total_rows = srep_citation_filtered.shape[0]\n",
    "print(f\"Total number of data rows: {total_rows}\")\n",
    "\n",
    "# Show the first 20 rows of the filtered DataFrame\n",
    "print(\"\\nFirst 20 rows of filtered data:\")\n",
    "print(srep_citation_filtered.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03390331-327c-4b52-abe5-6eb187f31ca7",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf9d49d-ef55-4a9d-8a8f-e87241d7f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'filtered_search_report_info.csv'\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Display the first three rows in an interactive DataTable\n",
    "    table_widget = widgets.Output()\n",
    "    with table_widget:\n",
    "        display(df.head(3))\n",
    "\n",
    "    display(table_widget)\n",
    "else:\n",
    "    print(f\"The file {file_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "accee5b8-c249-4607-b38f-dcb26c128ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the 'IPC' column\n",
    "def clean_ipc_column(ipc_data):\n",
    "    try:\n",
    "        # Convert string representation of list to actual list of dictionaries\n",
    "        ipc_list = ast.literal_eval(ipc_data)\n",
    "        \n",
    "        # Extract and clean the 'symbol', removing unwanted characters except alphanumeric and '/'\n",
    "        cleaned_ipc = [re.sub(r'[^A-Za-z0-9/]', '', entry['symbol']) for entry in ipc_list]\n",
    "        \n",
    "        # Join cleaned symbols into a single string, separated by commas\n",
    "        return ', '.join(cleaned_ipc)\n",
    "    except (ValueError, SyntaxError, KeyError):\n",
    "        # If any error occurs during the processing, return the original data\n",
    "        return ipc_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "192175c6-31f1-4af3-ac1d-c0113bb86ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the 'search_report_ipc_field' column\n",
    "def clean_search_report_ipc_field(ipc_data):\n",
    "    try:\n",
    "        # Convert string representation of list to actual list (if it's stored as a string)\n",
    "        ipc_list = ast.literal_eval(ipc_data)\n",
    "        \n",
    "        # Extract and clean the 'symbol', removing unwanted characters except alphanumeric and '/'\n",
    "        cleaned_ipc = [re.sub(r'[^A-Za-z0-9/]', '', entry) for entry in ipc_list]\n",
    "        \n",
    "        # Join cleaned symbols into a single string, separated by commas\n",
    "        return ', '.join(cleaned_ipc)\n",
    "    except (ValueError, SyntaxError, KeyError):\n",
    "        # If any error occurs during the processing, return the original data\n",
    "        return ipc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2c394c4c-d291-480e-93c5-7c157c675f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean up the srep_citation field\n",
    "def clean_srep_citation(citation_data):\n",
    "    try:\n",
    "        # Convert string representation of list to an actual list of dictionaries\n",
    "        citation_list = ast.literal_eval(citation_data)\n",
    "        \n",
    "        # Regular expression to extract the patent identifier from document_xml (e.g., US10976829B1)\n",
    "        patent_id_pattern = re.compile(r'dnum=\"([A-Z0-9]+)\"')\n",
    "\n",
    "        # Extract the patent identifier from the 'document_xml' field\n",
    "        cleaned_citations = []\n",
    "        for entry in citation_list:\n",
    "            if 'document_xml' in entry:\n",
    "                match = patent_id_pattern.search(entry['document_xml'])\n",
    "                if match:\n",
    "                    cleaned_citations.append(match.group(1))\n",
    "        \n",
    "        # Return a comma-separated string of patent identifiers\n",
    "        return ', '.join(cleaned_citations) if cleaned_citations else ''\n",
    "    \n",
    "    except (ValueError, SyntaxError, KeyError):\n",
    "        # If any error occurs during the processing, return an empty string to indicate no valid data\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1afecd35-3adc-4592-851e-2e4d59f8f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the document numbers from the cleaned srep_citation\n",
    "def extract_document_numbers(cleaned_citation):\n",
    "    try:\n",
    "        # Regular expression to match and capture document numbers (e.g., 10976829, 3757908)\n",
    "        document_number_pattern = re.compile(r'[A-Z]{2}(\\d+)[A-Z]\\d?')\n",
    "\n",
    "        # Find all document numbers in the cleaned srep_citation\n",
    "        document_numbers = document_number_pattern.findall(cleaned_citation)\n",
    "\n",
    "        # Return a comma-separated string of document numbers or an empty string if none found\n",
    "        return ', '.join(document_numbers) if document_numbers else ''\n",
    "    \n",
    "    except (ValueError, SyntaxError, KeyError):\n",
    "        # If any error occurs during the processing, return an empty string\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4a7969-4aad-45c4-bc7e-cd17f027d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths\n",
    "input_file_path = 'filtered_search_report_info.csv'\n",
    "output_file_path = 'cleaned_columns_output_prior_art.csv'\n",
    "document_numbers_file_path = 'documents_to_fetch.csv'\n",
    "\n",
    "# Set the chunk size for reading and processing the CSV file\n",
    "chunk_size = 5000\n",
    "\n",
    "# Process the CSV file in chunks\n",
    "chunks = pd.read_csv(input_file_path, chunksize=chunk_size)\n",
    "\n",
    "# Initialize the output CSV file with headers from the first chunk\n",
    "header_written = False\n",
    "document_header_written = False\n",
    "\n",
    "# Iterate through chunks\n",
    "for chunk in chunks:\n",
    "    # Apply the cleaning functions to the relevant columns\n",
    "    chunk['ipc'] = chunk['ipc'].apply(clean_ipc_column)\n",
    "    chunk['search_report_ipc_field'] = chunk['search_report_ipc_field'].apply(clean_search_report_ipc_field)\n",
    "    chunk['srep_citation'] = chunk['srep_citation'].apply(clean_srep_citation)\n",
    "    chunk['document_numbers'] = chunk['srep_citation'].apply(extract_document_numbers)\n",
    "\n",
    "    # Keep only the 'publication_number', 'ipc', 'search_report_ipc_field', 'srep_citation', and 'document_numbers' columns\n",
    "    cleaned_chunk = chunk[['publication_number', 'ipc', 'search_report_ipc_field', 'srep_citation', 'document_numbers']]\n",
    "\n",
    "    # Save the cleaned data\n",
    "    if not header_written:\n",
    "        cleaned_chunk.to_csv(output_file_path, index=False, mode='w')  # Write the header\n",
    "        header_written = True\n",
    "    else:\n",
    "        cleaned_chunk.to_csv(output_file_path, index=False, mode='a', header=False)  # Append without the header\n",
    "\n",
    "    # Save only the document numbers\n",
    "    document_numbers_chunk = chunk[['document_numbers']]\n",
    "    if not document_header_written:\n",
    "        document_numbers_chunk.to_csv(document_numbers_file_path, index=False, mode='w')  # Write the header\n",
    "        document_header_written = True\n",
    "    else:\n",
    "        document_numbers_chunk.to_csv(document_numbers_file_path, index=False, mode='a', header=False)  # Append without the header\n",
    "\n",
    "# Print a success message\n",
    "print(f\"Cleaned data saved to {output_file_path}!\")\n",
    "print(f\"Document numbers saved to {document_numbers_file_path}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b671e7af-7803-41f0-80ee-497345fe94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'cleaned_columns_output_prior_art.csv'\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Display the first three rows in an interactive DataTable\n",
    "    table_widget = widgets.Output()\n",
    "    with table_widget:\n",
    "        display(df.head(300))\n",
    "\n",
    "    display(table_widget)\n",
    "else:\n",
    "    print(f\"The file {file_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5defddbc-b08e-44a7-b1e7-577f1272fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path for the document numbers CSV\n",
    "document_numbers_file_path = 'documents_to_fetch.csv'\n",
    "\n",
    "# Read the CSV file containing the document numbers\n",
    "data = pd.read_csv(document_numbers_file_path)\n",
    "\n",
    "# Display initial number of rows\n",
    "initial_row_count = data.shape[0]\n",
    "print(f\"Initial number of rows: {initial_row_count}\")\n",
    "\n",
    "# Split the 'document_numbers' column into separate rows\n",
    "# First, convert the 'document_numbers' column into a list of strings by splitting on the commas\n",
    "data_expanded = data['document_numbers'].str.split(',', expand=True)\n",
    "\n",
    "# Melt the dataframe so that all the split values are in a single 'document_number' column\n",
    "data_melted = data_expanded.melt(value_name='document_number').dropna()\n",
    "\n",
    "# Remove leading and trailing spaces from document numbers\n",
    "data_melted['document_number'] = data_melted['document_number'].str.strip()\n",
    "\n",
    "# Drop rows with empty or missing document numbers\n",
    "data_cleaned = data_melted[['document_number']].dropna().replace('', pd.NA).dropna().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Display final number of rows\n",
    "final_row_count = data_cleaned.shape[0]\n",
    "print(f\"Final number of rows: {final_row_count}\")\n",
    "\n",
    "# Display the first 10 rows of the final output\n",
    "print(\"\\nFirst 10 rows of the final output:\")\n",
    "print(data_cleaned.head(10))\n",
    "\n",
    "# Optionally, you can save the cleaned document numbers into a new CSV file\n",
    "data_cleaned.to_csv('documents_to_fetch.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd2928-4b74-481a-a21f-8751ccf0e2d7",
   "metadata": {},
   "source": [
    "## Create Train Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7813b82e-3948-4f6e-b769-73699a4e4bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "file_path = 'documents_to_fetch.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract the document_number column\n",
    "document_numbers_df = df[['document_number']]\n",
    "\n",
    "# Initialize an empty DataFrame to store the search results in each batch\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# Counters for successful and unsuccessful fetches\n",
    "successful_fetches = 0\n",
    "unsuccessful_fetches = 0\n",
    "\n",
    "\n",
    "# Loop through the DataFrame and convert document_number to string\n",
    "for index, row in document_numbers_df.iterrows():\n",
    "    document_number = str(row['document_number'])  # Convert to string\n",
    "    \n",
    "    # Now use the document_number in your query\n",
    "    q = epab.query_publication(number=document_number, kind_code=\"\", date=\"\")\n",
    "    result = q.get_results(\"publication.number, ipc, claims\")\n",
    "    #result = q.get_results(\"publication.number, ipc, claims, abstract.text, description.text\")\n",
    "    df_data = pd.DataFrame(result)\n",
    "    # Check if df_data is not empty before appending\n",
    "    if not df_data.empty:\n",
    "        #print(\"Found Document for:\", document_number)\n",
    "        final_df = pd.concat([final_df, df_data], ignore_index=True)\n",
    "        successful_fetches += 1\n",
    "        #print (final_df.shape[0])\n",
    "    else:\n",
    "        unsuccessful_fetches += 1\n",
    "        \n",
    "    \n",
    "\n",
    "final_df.to_csv('fetched_documents.csv')\n",
    "# Print the total number of successful and unsuccessful fetches\n",
    "print(f\"Total successful fetches: {successful_fetches}\")\n",
    "print(f\"Total unsuccessful fetches: {unsuccessful_fetches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4109a0c2-d017-4710-b8a4-6f1c9b9f2c9a",
   "metadata": {},
   "source": [
    "# Create Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c441467-e053-4767-934f-ba9b29ed96fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set number of data you want to take\n",
    "number_of_data = 10000\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = 'cleaned_columns_output_prior_art.csv'\n",
    "df = pd.read_csv(file_path, nrows = number_of_data)\n",
    "\n",
    "\n",
    "\n",
    "# Extract the document_number column\n",
    "document_numbers_df = df[['publication_number']]\n",
    "\n",
    "# Initialize an empty DataFrame to store the search results in each batch\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# Counters for successful and unsuccessful fetches\n",
    "successful_fetches = 0\n",
    "unsuccessful_fetches = 0\n",
    "\n",
    "\n",
    "# Loop through the DataFrame and convert document_number to string\n",
    "for index, row in document_numbers_df.iterrows():\n",
    "    document_number = str(row['publication_number'])  # Convert to string\n",
    "    \n",
    "    # Now use the document_number in your query\n",
    "    q = epab.query_publication(number=document_number, kind_code=\"\", date=\"\", language='EN' )\n",
    "    result = q.get_results(\"publication.number, publication.language, ipc, claims\")\n",
    "    #result = q.get_results(\"publication.number, ipc, claims, abstract.text, description.text\")\n",
    "    df_data = pd.DataFrame(result)\n",
    "    # Check if df_data is not empty before appending\n",
    "    if not df_data.empty:\n",
    "        #print(\"Found Document for:\", document_number)\n",
    "        final_df = pd.concat([final_df, df_data], ignore_index=True)\n",
    "        successful_fetches += 1\n",
    "        #print (final_df.shape[0])\n",
    "    else:\n",
    "        unsuccessful_fetches += 1\n",
    "        \n",
    "    \n",
    "\n",
    "final_df.to_csv('prior_art_validation_dataset.csv')\n",
    "# Print the total number of successful and unsuccessful fetches\n",
    "print(f\"Total successful fetches: {successful_fetches}\")\n",
    "print(f\"Total unsuccessful fetches: {unsuccessful_fetches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f664ff9c-9574-4896-9827-1d47f576ec99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2656104-c2dd-4feb-a2ed-0619f688019c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
